"""
Ephyra Chatbot - Production RAG (Retrieval Augmented Generation)
LLM-Based Architecture with Semantic Search, Keyword Search, and Gemini Generation
"""

from functools import lru_cache
import os
import io
import uuid
import logging
from pathlib import Path
from typing import AsyncGenerator, Optional, List, Dict, Tuple
from datetime import datetime, timedelta
import json
import psycopg2
import psycopg2.pool
import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles

app = FasstAPI()
#Î•Î´Ï Î±ÎºÏÎ¹Î²ÏÏ‚ ÎºÏŒÎ»Î»Î·ÏƒÎµ Ï„Î¿ mount:
app.mount("/static", StaticFiles(directory="static"), name="static")

from pydantic import BaseModel
from dotenv import load_dotenv
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

from openai import OpenAI
from elevenlabs.client import ElevenLabs
import re

from sentence_transformers import SentenceTransformer
from langdetect import detect, LangDetectException
from fastapi.middleware.cors import CORSMiddleware
# ================== Configuration ==================
load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=True)

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
log = logging.getLogger("ephyra")

required_vars = ["OPENAI_API_KEY", "DB_NAME", "DB_USER", "DB_PASS", "DB_HOST"]
missing = [v for v in required_vars if not os.getenv(v)]
if missing:
    raise RuntimeError(f"Missing env vars: {', '.join(missing)}")

# Î•Î´Ï Ï€ÏÎ¿ÏƒÎ¸Î­Ï„ÎµÎ¹Ï‚ Ï„Î·Î½ Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î¿Ï… OpenAI
from openai import OpenAI
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# ================== FastAPI Setup ==================
app = FastAPI(title="Ephyra Chatbot - Production RAG", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
try:
    static_dir = os.path.dirname(os.path.abspath(__file__))
    app.mount("/static", StaticFiles(directory=static_dir, check_dir=True), name="static")
except Exception as e:
    log.warning(f"âš ï¸ Could not mount static files: {e}")

# ================== Database ==================
try:
    conn_pool = psycopg2.pool.SimpleConnectionPool(
        5, 20,
        dbname=os.getenv("DB_NAME"),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASS"),
        host=os.getenv("DB_HOST"),
        port=os.getenv("DB_PORT", "5432"),
    )
    log.info("âœ… Database connection pool created")
except Exception as e:
    log.exception("âŒ Database connection failed")
    raise

def get_db_conn():
    try:
        return conn_pool.getconn()
    except Exception as e:
        log.exception("âŒ Failed to get DB connection")
        raise

def return_db_conn(conn):
    if conn:
        conn_pool.putconn(conn)

# Alias Î³Î¹Î± ÏƒÏ…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î±
get_db_connection = get_db_conn

# ================== Embeddings (Lazy Load) ==================
embedder = None

@lru_cache(maxsize=1)
def get_embedder():
    global embedder
    if embedder is None:
        log.info("ğŸ“„ Loading SentenceTransformer (first use)...")
        embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        log.info("âœ… SentenceTransformer loaded")
    return embedder

# ================== ElevenLabs Setup ==================
from elevenlabs.client import ElevenLabs

ELEVENLABS_API_KEY = (os.getenv("ELEVENLABS_API_KEY") or "").strip()
ELEVENLABS_VOICE_ID = (os.getenv("ELEVENLABS_VOICE_ID") or "EXAVITQu4vr4xnSDxMaL").strip()
eleven_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
# ================== Pydantic Models ==================
class Message(BaseModel):
    role: str
    content: str

class AskBody(BaseModel):
    messages: List[Message]
    top_k: int = 10  # Changed from 5 to 10 for better coverage
    lang: str = "el"

class TTSBody(BaseModel):
    text: str

class FeedbackBody(BaseModel):
    bot_response: str
    user_question: str
    is_positive: bool
    conversation_id: str

# ================== Helper Functions ==================

def detect_user_lang(text: str) -> str:
    """Detect user language."""
    try:
        lang = detect((text or "").strip())
        return "en" if lang and lang.startswith("en") else "el"
    except LangDetectException:
        return "el"

def is_greeting(text: str) -> bool:
    """Check if text is a greeting."""
    greetings = ['Î³ÎµÎ¯Î±', 'Î³ÎµÎ¹Î±', 'ÎºÎ±Î»Î·Î¼Î­ÏÎ±', 'ÎºÎ±Î»Î·Î¼ÎµÏÎ±', 'ÎºÎ±Î»Î·ÏƒÏ€Î­ÏÎ±', 'ÎºÎ±Î»Î·ÏƒÏ€ÎµÏÎ±',
                 'Ï‡Î¬Î¹ÏÎµÏ„Îµ', 'Ï‡Î±Î¹ÏÎµÏ„Îµ', 'hello', 'hi', 'good morning', 'good evening']
    text_lower = text.lower().strip()
    return any(text_lower.startswith(g) or text_lower == g for g in greetings)

def get_direct_answer(question: str) -> Optional[Dict]:
    """Return direct answers for common questions that semantic search might miss."""
    text_lower = question.lower().strip()
    
    # ÎšÎ•Î  - ÎÏÎµÏ‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±Ï‚
    if any(kw in text_lower for kw in ['ÎºÎµÏ€', 'ÎºÎ­Î½Ï„ÏÎ¿ ÎµÎ¾Ï…Ï€Î·ÏÎ­Ï„Î·ÏƒÎ·Ï‚ Ï€Î¿Î»Î¹Ï„ÏÎ½', 'center']):
        if any(kw in text_lower for kw in ['ÏÏÎµÏ‚', 'Ï‰ÏÎ¬ÏÎ¹Î¿', 'Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³', 'hours', 'time', 'when', 'Ï€ÏŒÏ„Îµ']):
            return {
                "answer": """ÎšÎ•Î  ÎšÎ¿ÏÎ¯Î½Î¸Î¿Ï… - Î©ÏÎ¬ÏÎ¹Î¿ Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±Ï‚

ğŸ“ Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·: ÎšÏ‰ÏƒÏ„Î® Î Î±Î»Î±Î¼Î¬ 53, 20131 ÎšÏŒÏÎ¹Î½Î¸Î¿Ï‚

ğŸ•’ Î©ÏÎ¬ÏÎ¹Î¿: Î”ÎµÏ…Ï„Î­ÏÎ± - Î Î±ÏÎ±ÏƒÎºÎµÏ…Î®, 8:00 - 15:00
         (Î¤ÎµÏ„Î¬ÏÏ„Î· ÎµÏ€Î¯ÏƒÎ·Ï‚ 17:00 - 19:00)

ğŸ“ Î¤Î·Î»Î­Ï†Ï‰Î½Î¿: 2741363555
ğŸ“§ Email: n.korinthias@kep.gov.gr""",
                "quality": "direct_match",
                "context_found": True,
                "confidence": 0.95
            }
        # ÎšÎ•Î  - Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±Ï‚
        if any(kw in text_lower for kw in ['Ï„Î·Î»Î­Ï†Ï‰Î½Î¿', 'Ï„Î·Î»', 'email', 'address', 'Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·', 'ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±', 'ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±']):
            return {
                "answer": """ÎšÎ•Î  ÎšÎ¿ÏÎ¯Î½Î¸Î¿Ï… - Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±Ï‚

ğŸ“ Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·: ÎšÏ‰ÏƒÏ„Î® Î Î±Î»Î±Î¼Î¬ 53, 20131 ÎšÏŒÏÎ¹Î½Î¸Î¿Ï‚

ğŸ“ Î¤Î·Î»Î­Ï†Ï‰Î½Î¿: 2741363555
ğŸ“§ Email: n.korinthias@kep.gov.gr

ğŸ•’ Î©ÏÎ¬ÏÎ¹Î¿: Î”ÎµÏ…Ï„Î­ÏÎ± - Î Î±ÏÎ±ÏƒÎºÎµÏ…Î® 8:00-15:00""",
                "quality": "direct_match",
                "context_found": True,
                "confidence": 0.95
            }
    
    # Î”Î•Î¥Î‘ - Multiple variations
    if any(kw in text_lower for kw in ['Î´ÎµÏ…Î±', 'Î´.Îµ.Ï….Î±', 'water', 'Î½ÎµÏÏŒ']):
        if any(kw in text_lower for kw in ['Ï„Î·Î»Î­Ï†Ï‰Î½Î¿', 'Ï„Î·Î»', 'ÎºÎ»Î®ÏƒÎ·', 'ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±', 'call', 'phone']):
            return {
                "answer": """Î”Î•Î¥Î‘ ÎšÎ¿ÏÎ¯Î½Î¸Î¿Ï…
ğŸ“ Î¤Î·Î»Î­Ï†Ï‰Î½Î¿ ÎšÎ­Î½Ï„ÏÎ¿: 2741024444
ğŸ“ Î’Î»Î¬Î²ÎµÏ‚ (24Ï‰ÏÎ¿): 6936776041
ğŸ“§ Email: info@deyakor.gr""",
                "quality": "direct_match",
                "context_found": True,
                "confidence": 0.95
            }
    
    # Î”Î®Î¼Î±ÏÏ‡Î¿Ï‚ - Multiple variations
    if any(kw in text_lower for kw in ['Î´Î®Î¼Î±ÏÏ‡', 'Î´Î·Î¼Î±ÏÏ‡', 'mayor', 'Î±ÏÏ‡Î·Î³']):
        if any(kw in text_lower for kw in ['Ï„Î·Î»Î­Ï†Ï‰Î½Î¿', 'Ï„Î·Î»', 'email', 'ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±', 'ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±', 'contact']):
            return {
                "answer": """Î“ÏÎ±Ï†ÎµÎ¯Î¿ Î”Î·Î¼Î¬ÏÏ‡Î¿Ï… ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½

Î”Î®Î¼Î±ÏÏ‡Î¿Ï‚: ÎÎ¯ÎºÎ¿Ï‚ Î£Ï„Î±Ï…ÏÎ­Î»Î·Ï‚

ğŸ“ Î¤Î·Î»Î­Ï†Ï‰Î½Î¿: 27413-61001, 27413-61041
ğŸ“§ Email: grafeiodimarxou@korinthos.gr

ğŸ“ Î”Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·: ÎšÎ¿Î»Î¹Î¬Ï„ÏƒÎ¿Ï… 32, 201 31 ÎšÏŒÏÎ¹Î½Î¸Î¿Ï‚""",
                "quality": "direct_match",
                "context_found": True,
                "confidence": 0.95
            }
    
    # Î¤Î·Î»Î­Ï†Ï‰Î½Î± Î”Î®Î¼Î¿Ï… - Multiple variations
    if any(kw in text_lower for kw in ['Ï„Î·Î»Î­Ï†Ï‰Î½Î± Î´Î®Î¼Î¿Ï…', 'Î´Î·Î¼Î¿ÏƒÎ¹Î¿ Ï„Î·Î»', 'Î´Î·Î¼Î¿Ï‚ ÎºÎ¿ÏÎ¹Î½Î¸', 'ÎºÎ»Î®ÏƒÎ· Î´Î®Î¼Î¿Ï…', 'phone municipality']):
        if any(kw in text_lower for kw in ['Ï„Î·Î»', 'ÎºÎ­Î½Ï„ÏÎ¿', 'ÎºÎ»Î®ÏƒÎ·', 'phone', 'call', 'contact']):
            return {
                "answer": """Î¤Î·Î»ÎµÏ†Ï‰Î½Î¹ÎºÏŒ ÎšÎ­Î½Ï„ÏÎ¿ Î”Î®Î¼Î¿Ï… ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½
ğŸ“ 27413-61000 (ÎšÏÏÎ¹Î± Î³ÏÎ±Î¼Î¼Î®)
ğŸ“ 27413-61040 (Î•Î½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ®)
ğŸ“ 27413-61045 (Î“ÏÎ±Ï†ÎµÎ¯Î¿ Î¤ÏÏ€Î¿Ï…)

Î©ÏÎ¬ÏÎ¹Î¿: Î”ÎµÏ…Ï„Î­ÏÎ±-Î Î±ÏÎ±ÏƒÎºÎµÏ…Î® 8:00-14:00

Î“Î¹Î± Î±Î¹Ï„Î®Î¼Î±Ï„Î±: protokollo@korinthos.gr""",
                "quality": "direct_match",
                "context_found": True,
                "confidence": 0.95
            }
    
    return None
    """Check if user is asking about bot's capabilities."""
    text_lower = text.lower().strip()
    capability_keywords = [
        'Ï„Î¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚', 'Ï€Î¿Î¹ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚', 'Ï„Î¹ Î¼Ï€Î¿ÏÎµÎ¹Ï‚', 'Ï„Î¹ Î´Ï…Î½Î±Ï„Î¿Ï„Î·Ï„ÎµÏ‚',
        'Î¼Ï€Î¿ÏÎµÎ¹Ï‚ Î½Î± ÎºÎ±Î½ÎµÎ¹Ï‚', 'Î¼Ï€Î¿ÏÎµÎ¹Ï‚ Î½Î± Î¼Î¿Ï… Ï€Î±ÏÎµÏ‡ÎµÎ¹Ï‚', 'Î²Î¿Î·Î¸Î±', 'Î²Î¿Î·Î¸Î·ÏƒÎµÎ¹Ï‚',
        'what can you help', 'what information', 'what capabilities', 'can you help'
    ]
    return any(kw in text_lower for kw in capability_keywords)

def get_capabilities_response(lang: str = "el") -> str:
    """Return bot capabilities response - compact version with titles only."""
    if lang == "en":
        return """As a digital assistant for the Municipality of Corinth, I can help with:

1) Municipal Phone Numbers
2) Certificate Issuance
3) Registry Acts
4) Municipal History
5) General Municipal Services

For more information, visit https://korinthos.gr/"""
    else:
        return """Î©Ï‚ ÏˆÎ·Ï†Î¹Î±ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ Ï„Î¿Ï… Î”Î®Î¼Î¿Ï… ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½, Î¼Ï€Î¿ÏÏ Î½Î± ÏƒÎ±Ï‚ Î´ÏÏƒÏ‰ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î±:

1) Î¤Î·Î»Î­Ï†Ï‰Î½Î± Ï„Î¿Ï… Î”Î®Î¼Î¿Ï…
2) ÎˆÎºÎ´Î¿ÏƒÎ· Î Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Ï„Î¹ÎºÏÎ½
3) Î›Î·Î¾Î¹Î±ÏÏ‡Î¹ÎºÎ­Ï‚ Î ÏÎ¬Î¾ÎµÎ¹Ï‚
4) Î™ÏƒÏ„Î¿ÏÎ¯Î± Ï„Î¿Ï… Î”Î®Î¼Î¿Ï…
5) Î”Î·Î¼Î¿Ï„Î¹ÎºÎ­Ï‚ Î¥Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚

Î“Î¹Î± Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, ÎµÏ€Î¹ÏƒÎºÎµÏ†Î¸ÎµÎ¯Ï„Îµ https://korinthos.gr/"""

def is_capabilities_question(text: str) -> bool:
    """Check if user is asking about bot's capabilities."""
    text_lower = text.lower().strip()
    capability_keywords = [
        'Ï„Î¹ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚', 'Ï€Î¿Î¹ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚', 'Ï„Î¹ Î¼Ï€Î¿ÏÎµÎ¹Ï‚', 'Ï„Î¹ Î´Ï…Î½Î±Ï„Î¿Ï„Î·Ï„ÎµÏ‚',
        'Î¼Ï€Î¿ÏÎµÎ¹Ï‚ Î½Î± ÎºÎ±Î½ÎµÎ¹Ï‚', 'Î¼Ï€Î¿ÏÎµÎ¹Ï‚ Î½Î± Î¼Î¿Ï… Ï€Î±ÏÎµÏ‡ÎµÎ¹Ï‚', 'Î²Î¿Î·Î¸Î±', 'Î²Î¿Î·Î¸Î·ÏƒÎµÎ¹Ï‚',
        'what can you help', 'what information', 'what capabilities', 'can you help',
        'Ï„Î¹ Î¾ÎµÏÎµÎ¹Ï‚', 'Ï„Î¹ Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹Ï‚', 'Î³Î¹Î± Ï„Î¹', 'ÏƒÏ‡ÎµÏ„Î¹ÎºÎ± Î¼Îµ Ï„Î¹'
    ]
    return any(kw in text_lower for kw in capability_keywords)

def is_out_of_scope(text: str) -> bool:
    """Check if question is out of scope. Be lenient - let RAG decide."""
    text_lower = text.lower().strip()
    
    # Scope keywords - if ANY of these are present, it's IN scope
    scope_keywords = [
        'Î´Î®Î¼Î¿Ï‚', 'ÎºÎ¿ÏÎ¯Î½Î¸', 'Ï„Î·Î»Î­Ï†Ï‰Î½Î¿', 'ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±', 'Ï€Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Ï„Î¹ÎºÏŒ',
        'Î³Î­Î½Î½Î·ÏƒÎ·Ï‚', 'Î¸Î±Î½Î¬Ï„Î¿Ï…', 'Î³Î¬Î¼Î¿Ï…', 'Î»Î·Î¾Î¹Î±ÏÏ‡Î¹ÎºÎ­Ï‚', 'Î¼ÎµÏ„Î±Î´Î·Î¼ÏŒÏ„ÎµÏ…ÏƒÎ·',
        'Ï…Ï€Î·ÏÎµÏƒÎ¯Î±', 'Î´Î·Î¼Î±ÏÏ‡', 'Î±Î¯Ï„Î·ÏƒÎ·', 'Î­Î³Î³ÏÎ±Ï†Î¿', 'Ï‰ÏÎ¬ÏÎ¹Î¿', 'gov.gr',
        'ÎµÎºÎ´Î¿ÏƒÎ·', 'Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ±', 'Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±', 'Ï€ÏÎ¿Ï‹Ï€Î¿Î¸Î­ÏƒÎµÎ¹Ï‚', 'Î²Î®Î¼Î±Ï„Î±',
        'municipality', 'corinth', 'certificate', 'service', 'process'
    ]
    
    # If ANY keyword is present, it's likely IN scope - let RAG decide
    if any(kw in text_lower for kw in scope_keywords):
        return False  # IN SCOPE - let RAG handle it
    
    # Very obviously out of scope
    clearly_out_of_scope = [
        'weather', 'ÎºÎ±Î¹ÏÏŒÏ‚', 'football', 'Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿', 'recipe', 'ÏƒÏ…Î½Ï„Î±Î³Î®',
        'movie', 'Ï„Î±Î¹Î½Î¯Î±', 'politics', 'Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®', 'celebrity', 'ÏƒÎµÎ»Î­Î¼Ï€ÏÎ¹Ï„Î¹'
    ]
    
    if any(kw in text_lower for kw in clearly_out_of_scope):
        return True  # OUT OF SCOPE
    
    # When in doubt, let RAG try - it's better to attempt than reject
    return False

def semantic_search(cursor, question: str, top_k: int = 5) -> List[Dict]:
    """
    Semantic search using embeddings.
    Returns top-k most relevant documents.
    """
    try:
        q_embedding = get_embedder().encode(question)
        q_embedding_list = q_embedding.tolist()
        
        # Get MORE results initially to have better selection
        cursor.execute("""
            SELECT id, question, answer, 
                   1 - (embedding_384 <=> %s::vector) as similarity
            FROM public.kb_items_raw 
            WHERE embedding_384 IS NOT NULL
            ORDER BY embedding_384 <-> %s::vector
            LIMIT %s
        """, (q_embedding_list, q_embedding_list, top_k * 3))  # Get 3x more initially
        
        results = []
        for r_id, r_question, r_answer, similarity in cursor.fetchall():
            if similarity > 0.0:  # Accept ANY similarity (will filter later)
                results.append({
                    "id": r_id,
                    "question": r_question,
                    "answer": r_answer,
                    "similarity": float(similarity),
                    "source": "semantic"
                })
        
        # Return top-k sorted by similarity
        results = sorted(results, key=lambda x: x['similarity'], reverse=True)[:top_k]
        
        top_sims = [f"{r['similarity']:.3f}" for r in results[:3]]
        log.info(f"ğŸ” Semantic search: {len(results)} results (top similarities: {top_sims})")
        return results
    
    except Exception as e:
        log.error(f"âŒ Semantic search error: {e}")
        return []

def keyword_search(cursor, question: str, top_k: int = 3) -> List[Dict]:
    """
    Keyword search for exact matches.
    Useful when semantic search might miss specific terms.
    """
    try:
        q_lower = question.lower().strip()
        # Remove punctuation
        q_lower = q_lower.replace('?', '').replace(';', '').replace('!', '').replace(',', '')
        keywords = [kw.strip() for kw in q_lower.split() if len(kw.strip()) > 2]
        
        if not keywords:
            return []
        
        # Build OR conditions for all keywords
        conditions = " OR ".join([f"(LOWER(question) ILIKE %s OR LOWER(answer) ILIKE %s)" for _ in keywords])
        params = []
        for kw in keywords:
            params.extend([f"%{kw}%", f"%{kw}%"])
        params.append(top_k * 2)  # Get more results
        
        cursor.execute(f"""
            SELECT id, question, answer, 0.95 as similarity
            FROM public.kb_items_raw 
            WHERE {conditions}
            ORDER BY id
            LIMIT %s
        """, params)
        
        results = []
        seen_ids = set()
        for r_id, r_question, r_answer, similarity in cursor.fetchall():
            if r_id not in seen_ids:
                results.append({
                    "id": r_id,
                    "question": r_question,
                    "answer": r_answer,
                    "similarity": float(similarity),
                    "source": "keyword"
                })
                seen_ids.add(r_id)
        
        log.info(f"ğŸ”‘ Keyword search: {len(results)} results (keywords: {keywords})")
        return results
    
    except Exception as e:
        log.error(f"âŒ Keyword search error: {e}")
        return []

def retrieve_context(cursor, question: str, top_k: int = 10) -> List[Dict]:
    """
    RAG Step 1: RETRIEVE
    Combines semantic + keyword search, ranks and deduplicates results.
    Returns MORE documents so Gemini can find the best match regardless of wording.
    """
   # 1. Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎºÎ±Î¹ Ï„Ï‰Î½ Î´ÏÎ¿ Î±Î½Î±Î¶Î·Ï„Î®ÏƒÎµÏ‰Î½ (Semantic & Keyword)
    # Î‘Ï…Î¾Î¬Î½Î¿Ï…Î¼Îµ Ï„Î¿ top_k Î³Î¹Î± Î½Î± Î­Ï‡Î¿Ï…Î¼Îµ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î·Î½ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±
    semantic_results = semantic_search(cursor, question, top_k=20)
    top_sim = semantic_results[0]['similarity'] if semantic_results else 0
    
    keyword_results = keyword_search(cursor, question, top_k=20)

    # 2. Î¥Î’Î¡Î™Î”Î™ÎšÎŸÎ£ Î£Î¥ÎÎ”Î¥Î‘Î£ÎœÎŸÎ£ (Hybrid Search) - Î§Î©Î¡Î™Î£ Î Î•Î¡Î™ÎŸÎ¡Î™Î£ÎœÎŸÎ¥Î£
    # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ dictionary Î³Î¹Î± Î½Î± ÎµÎ½ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï‡Ï‰ÏÎ¯Ï‚ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Î±
    all_results = {}
    
    # Î ÏÏÏ„Î± Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î±Ï€ÏŒ Ï„Î·Î½ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î»Î­Î¾ÎµÏ‰Î½-ÎºÎ»ÎµÎ¹Î´Î¹ÏÎ½
    for doc in keyword_results:
        all_results[doc['id']] = doc
    
    # ÎœÎµÏ„Î¬ Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î±Ï€ÏŒ Ï„Î· ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·
    for doc in semantic_results:
        if doc['id'] not in all_results:
            all_results[doc['id']] = doc
        else:
            # Î‘Î½ Î­Î½Î± Î­Î³Î³ÏÎ±Ï†Î¿ Î²ÏÎ­Î¸Î·ÎºÎµ ÎºÎ±Î¹ Î±Ï€ÏŒ Ï„Î¿Ï…Ï‚ Î´ÏÎ¿ Ï„ÏÏŒÏ€Î¿Ï…Ï‚, ÎºÏÎ±Ï„Î¬Î¼Îµ Ï„Î¿ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ similarity
            current_sim = doc.get('similarity', 0)
            existing_sim = all_results[doc['id']].get('similarity', 0)
            all_results[doc['id']]['similarity'] = max(current_sim, existing_sim)
    
    # ÎšÎ±Ï„Î±Î³ÏÎ±Ï†Î® Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÏÎ½ ÏƒÏ„Î¿ Log Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿ ÏƒÏ„Î¿ Ï„ÎµÏÎ¼Î±Ï„Î¹ÎºÏŒ ÏƒÎ¿Ï…
    log.info(f"ğŸ“‚ Hybrid Search: Top Semantic Sim: {top_sim:.3f}")
    log.info(f"ğŸ“š Total unique documents collected for GPT: {len(all_results)}")

    # 3. Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î²Î¬ÏƒÎµÎ¹ ÏƒÏ…Î½Î¬Ï†ÎµÎ¹Î±Ï‚ ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ¿Ï†Î®
    # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ top_k Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Î¿ÏÎ¹ÏƒÏ„ÎµÎ¯ (ÏƒÏ…Î½Î®Î¸Ï‰Ï‚ 10-15 Î³Î¹Î± Ï„Î¿ GPT)
    ranked = sorted(all_results.values(), key=lambda x: x.get('similarity', 0), reverse=True)[:top_k]
    
    # Î•Î Î™Î£Î¤Î¡ÎŸÎ¦Î— Î¤Î—Î£ Î›Î™Î£Î¤Î‘Î£ Î£Î¤Î— Î£Î¥ÎÎ‘Î¡Î¤Î—Î£Î— generate_answer_with_rag
    return ranked

def format_context(docs: List[Dict], lang: str = "el") -> str:
    """Format retrieved documents for the LLM prompt."""
    if not docs:
        if lang == "en":
            return "(No relevant documents found in the knowledge base)"
        else:
            return "(Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î­Î³Î³ÏÎ±Ï†Î± ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î³Î½ÏÏƒÎ·Ï‚)"
    
    if lang == "en":
        formatted = "Knowledge Base Documents:\n" + "="*60 + "\n"
    else:
        formatted = "ÎˆÎ³Î³ÏÎ±Ï†Î± Î±Ï€ÏŒ Ï„Î· Î’Î¬ÏƒÎ· Î“Î½ÏÏƒÎ·Ï‚:\n" + "="*60 + "\n"
    
    for i, doc in enumerate(docs, 1):
        formatted += f"\n[Document {i}] (Relevance: {doc['similarity']:.0%})\n"
        formatted += f"Q: {doc['question']}\n"
        formatted += f"A: {doc['answer']}\n"
    
    return formatted

async def generate_answer_with_rag(question: str, context_docs: List[Dict], 
                                   lang: str = "el", conversation_history: List[Message] = None) -> Tuple[str, Dict]:
    """
    Î Î±ÏÎ±Î³Ï‰Î³Î® AI Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î¼Îµ Ï€Î»Î®ÏÎ· Î±Î¾Î¹Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î¿Ï… GPT-4o-mini ÎºÎ±Î¹ Ï„Ï‰Î½ 88 Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÏÎ½.
    """
    log.info(f"ğŸ¤– Generating AI response in '{lang}' (Found {len(context_docs)} relevant docs)...")
    
    # 1. Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿ Formatting Ï„Î¿Ï… Context Î³Î¹Î± Î½Î± Ï„Î¿ "Î²Î»Î­Ï€ÎµÎ¹" ÎºÎ±Î¸Î±ÏÎ¬ Ï„Î¿ GPT
    if not context_docs:
        context_str = "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î­Î³Î³ÏÎ±Ï†Î± ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½."
    else:
        # Î•Î½ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ ÏƒÎµ Î¼Î¹Î± ÎºÎ±Î¸Î±ÏÎ® Î»Î¯ÏƒÏ„Î±
         # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Ï‰Î½ docs ÏƒÎµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï€Î¿Ï… Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ Î· AI
          context_str = "\n".join([f"Î•ÏÏÏ„Î·ÏƒÎ·: {d.get('question', '')} - Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·: {d.get('answer', '')}" for d in context_docs])

    # 2. Î¤Î¿ "Î•Î»ÎµÏÎ¸ÎµÏÎ¿" Î±Î»Î»Î¬ "Î ÎµÎ¹Î¸Î±ÏÏ‡Î·Î¼Î­Î½Î¿" System Prompt
    if lang == "el":
        system_prompt = (
         "Î•Î¯ÏƒÎ±Î¹ Î· Î•Ï†ÏÏÎ±, Î· Ï€ÏÎ¿Î·Î³Î¼Î­Î½Î· AI Î²Î¿Î·Î¸ÏŒÏ‚ Ï„Î¿Ï… Î”Î®Î¼Î¿Ï… ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½. Î— Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® ÏƒÎ¿Ï… ÎµÎ¯Î½Î±Î¹ Î½Î± ÎµÎ¾Ï…Ï€Î·ÏÎµÏ„ÎµÎ¯Ï‚ Ï„Î¿Ï…Ï‚ Ï€Î¿Î»Î¯Ï„ÎµÏ‚ "
    "Î¼Îµ Ï†Ï…ÏƒÎ¹ÎºÏŒ, Î±Î½Î¸ÏÏÏ€Î¹Î½Î¿ ÎºÎ±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏÏŒÏ€Î¿, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Ï„Î·Î½ ÎµÏ€Î¯ÏƒÎ·Î¼Î· Î²Î¬ÏƒÎ· Î³Î½ÏÏƒÎ·Ï‚ Ï„Î¿Ï… Î”Î®Î¼Î¿Ï….\n\n"
    
    # Î— ÎšÎ¡Î™Î£Î™ÎœÎ— Î Î¡ÎŸÎ£Î˜Î—ÎšÎ— Î“Î™Î‘ Î¤Î— Î“Î›Î©Î£Î£Î‘:
    "Î“Î›Î©Î£Î£Î™ÎšÎŸÎ£ ÎšÎ‘ÎÎŸÎÎ‘Î£: Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ Î Î‘ÎÎ¤Î‘ ÏƒÏ„Î· Î³Î»ÏÏƒÏƒÎ± Ï€Î¿Ï… ÏƒÎ¿Ï… Î±Ï€ÎµÏ…Î¸ÏÎ½ÎµÏ„Î±Î¹ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚. "
    "Î‘Î½ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ ÏÏ‰Ï„Î®ÏƒÎµÎ¹ ÏƒÏ„Î± Î‘Î³Î³Î»Î¹ÎºÎ¬, Î¼ÎµÏ„Î­Ï†ÏÎ±ÏƒÎµ Ï„Î¹Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Ï„Î¿ ÎµÎ»Î»Î·Î½Î¹ÎºÏŒ CONTEXT ÎºÎ±Î¹ Î±Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± Î‘Î³Î³Î»Î¹ÎºÎ¬. "
    "Î‘Î½ ÏÏ‰Ï„Î®ÏƒÎµÎ¹ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬, Î±Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬.\n\n"

    "ÎšÎ‘ÎÎŸÎÎ•Î£ Î›Î•Î™Î¤ÎŸÎ¥Î¡Î“Î™Î‘Î£:\n"
    "1. Î Î¡ÎŸÎ¤Î•Î¡Î‘Î™ÎŸÎ¤Î—Î¤Î‘ Î”Î•Î”ÎŸÎœÎ•ÎÎ©Î: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÎœÎŸÎÎŸ Ï„Î¹Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Ï€Î¿Ï… Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ CONTEXT. "
    "Î‘Î³Î½ÏŒÎ·ÏƒÎµ Î¿Ï€Î¿Î¹Î±Î´Î®Ï€Î¿Ï„Îµ Ï€ÏÎ¿Ï‹Ï€Î¬ÏÏ‡Î¿Ï…ÏƒÎ± Î³Î½ÏÏƒÎ· Î±Ï€ÏŒ Ï„Î·Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ® ÏƒÎ¿Ï… Ï€Î¿Ï… Î­ÏÏ‡ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏÎ³ÎºÏÎ¿Ï…ÏƒÎ· (Ï€.Ï‡. Ï€Î±Î»Î¹Î¿ÏÏ‚ Î´Î·Î¼Î¬ÏÏ‡Î¿Ï…Ï‚). "
    "Î“Î¹Î± ÎµÏƒÎ­Î½Î±, Î”Î®Î¼Î±ÏÏ‡Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¿ ÎÎ™ÎšÎŸÎ£ Î£Î¤Î‘Î¥Î¡Î•Î›Î—Î£.\n"
    
    "2. Î‘ÎšÎ¡Î™Î’Î•Î™Î‘ Î¤Î—Î›Î•Î¦Î©ÎÎ©Î: ÎœÎ·Î½ ÎµÏ€Î¹Î½Î¿ÎµÎ¯Ï‚ Ï€Î¿Ï„Î­ Ï„Î·Î»ÎµÏ†Ï‰Î½Î¹ÎºÎ¬ Î½Î¿ÏÎ¼ÎµÏÎ±. Î‘Î½ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ ÏÏ‰Ï„Î¬ÎµÎ¹ Î³Î¹Î± Î²Î»Î¬Î²ÎµÏ‚ Î® Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚, "
    "Î´ÏÏƒÎµ Ï„Î¿ Î±ÎºÏÎ¹Î²Î­Ï‚ Î½Î¿ÏÎ¼ÎµÏÎ¿ Ï€Î¿Ï… Î±Î½Î±Î³ÏÎ¬Ï†ÎµÏ„Î±Î¹ ÏƒÏ„Î¿ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ (Ï€.Ï‡. Î³Î¹Î± Î·Î»ÎµÎºÏ„ÏÎ¿Ï†Ï‰Ï„Î¹ÏƒÎ¼ÏŒ Ï„Î¿ 2741120134). "
    "Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎµÎ¹Î´Î¹ÎºÏŒ Î½Î¿ÏÎ¼ÎµÏÎ¿, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¿ Î³ÎµÎ½Î¹ÎºÏŒ ÎºÎ­Î½Ï„ÏÎ¿ 2741361000.\n"
    
    "3. Î¦Î¥Î£Î™ÎšÎŸÎ£ Î›ÎŸÎ“ÎŸÎ£ (AI-Powered): ÎœÎ·Î½ Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ Î ÎŸÎ¤Î• Ï„Î¹Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ 'Context', 'Î­Î³Î³ÏÎ±Ï†Î±' Î® 'Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½'. "
    "ÎœÎ·Î½ Î»ÎµÏ‚ 'Î£ÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Ï„Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ 1'. Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ Î±Ï€ÎµÏ…Î¸ÎµÎ¯Î±Ï‚: 'ÎœÎµ Î²Î¬ÏƒÎ· Ï„Î·Î½ ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· Ï„Î¿Ï… Î”Î®Î¼Î¿Ï…...' Î® 'ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± ÎºÎ±Î»Î­ÏƒÎµÏ„Îµ ÏƒÏ„Î¿...'. "
    "Î£ÏÎ½Î¸ÎµÏƒÎµ Î¼Î¹Î± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï€Î¿Ï… ÏÎ­ÎµÎ¹ Ï†Ï…ÏƒÎ¹ÎºÎ¬, ÏƒÏ…Î½Î´Ï…Î¬Î¶Î¿Î½Ï„Î±Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Î½ Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯.\n"
    
    "4. Î”Î™Î‘Î§Î•Î™Î¡Î™Î£Î— Î Î‘Î¡Î‘Î›Î›Î‘Î“Î©Î: ÎšÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎµ Ï„Î¿ Î½ÏŒÎ·Î¼Î± Ï„Î·Ï‚ ÎµÏÏÏ„Î·ÏƒÎ·Ï‚. Î‘Î½ ÎºÎ¬Ï€Î¿Î¹Î¿Ï‚ ÏÏ‰Ï„Î®ÏƒÎµÎ¹ 'Ï€Î¿Î¹Î¿Ï‚ ÎºÎ¬Î½ÎµÎ¹ ÎºÎ¿Ï…Î¼Î¬Î½Ï„Î¿' "
    "Î® 'Ï€Î¿Î¹Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¿ Î±ÏÏ‡Î·Î³ÏŒÏ‚', ÎºÎ±Ï„Î¬Î»Î±Î²Îµ ÏŒÏ„Î¹ ÎµÎ½Î½Î¿ÎµÎ¯ Ï„Î¿Î½ Î”Î®Î¼Î±ÏÏ‡Î¿.\n"
    
    "5. Î‘Î“ÎÎ©Î£Î¤Î— Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î‘: Î‘Î½ Î· Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÏƒÏ„Î¹Ï‚ 88 ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚, Î¼Î·Î½ Î¼Î±Î½Ï„Î­ÏˆÎµÎ¹Ï‚. "
    "Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÎµÏ…Î³ÎµÎ½Î¹ÎºÎ¬ ÏŒÏ„Î¹ Î´ÎµÎ½ Î´Î¹Î±Î¸Î­Ï„ÎµÎ¹Ï‚ Ï„Î· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î± ÎºÎ±Î¹ Ï€Î±ÏÎ­Ï€ÎµÎ¼ÏˆÎµ ÏƒÏ„Î¿ korinthos.gr Î® ÏƒÏ„Î¿ 2741361000."
)
        
    else:
        system_prompt = (
            "You are Ephyra, the advanced AI assistant for the Municipality of Corinth. "
            "Your mission is to serve citizens in a natural, human, and professional manner, "
            "using EXCLUSIVELY the official knowledge base of the Municipality.\n\n"
            
            "OPERATIONAL RULES:\n"
            "1. DATA PRIORITY: Use ONLY the information provided in the CONTEXT. "
            "Ignore any prior knowledge from your training that conflicts with this data (e.g., former mayors). "
            "For you, the Mayor is NIKOS STAVRELIS.\n"
            
            "2. PHONE ACCURACY: Never invent phone numbers. If the user asks about faults or services, "
            "provide the exact number mentioned in the corresponding document (e.g., for street lighting use 2741120134). "
            "If no specific number is found, use the general center 2741361000.\n"
            
            "3. NATURAL LANGUAGE (AI-Powered): NEVER mention the words 'Context', 'documents', or 'database'. "
            "Do not say 'According to document 1'. Answer directly: 'Based on the Municipality's information...' "
            "or 'You can call...'. Compose a response that flows naturally, combining information if necessary.\n"
            
            "4. LANGUAGE RULE: Always answer in the language the user addresses you in. "
            "Since the CONTEXT is in Greek, you must accurately translate the information into English for the user.\n"
            
            "5. HANDLING VARIATIONS: Understand the meaning of the question. If someone asks 'who is in charge' "
            "or 'who is the boss', understand they mean the Mayor.\n"
            
            "6. UNKNOWN INFORMATION: If the information is not in the 88 records, do not guess. "
            "Politely state that you do not have this information and refer them to korinthos.gr or 2741361000."
        )
        

    try:
        # 3. ÎšÎ»Î®ÏƒÎ· OpenAI Î¼Îµ GPT-4o-mini
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "system", "content": f"Î”Î™Î‘Î˜Î•Î£Î™ÎœÎ•Î£ Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î•Î£ (CONTEXT):\n{context_str}"},
                {"role": "user", "content": question}
            ],
            temperature=0.7 # Î“Î¹Î± Ï†Ï…ÏƒÎ¹ÎºÏŒÏ„Î·Ï„Î± ÏƒÏ„Î¿Î½ Î»ÏŒÎ³Î¿
        )
        answer = response.choices[0].message.content.strip()
        
        # 4. Metadata Î³Î¹Î± Ï„Î¿ UI
        metadata = {
            "documents_used": len(context_docs),
            "avg_similarity": sum(d.get('similarity', 0) for d in context_docs) / len(context_docs) if context_docs else 0,
            "sources": list(set(d.get('source', 'Î’Î¬ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½') for d in context_docs))
        }
        return answer, metadata

    except Exception as e:
        log.error(f"âŒ OpenAI Error: {e}")
        error_msg = "Î›Ï…Ï€Î¬Î¼Î±Î¹, Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎµ ÏƒÏ†Î¬Î»Î¼Î± ÏƒÏ„Î· ÏƒÏÎ½Î´ÎµÏƒÎ· Î¼Îµ Ï„Î¿ AI." if lang == "el" else "Sorry, an AI error occurred."
        return error_msg, {"error": str(e)}

# ================== Endpoints ==================

@app.get("/")
async def root():
    """Serve HTML UI."""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    html_path = os.path.join(current_dir, "ui_chatbot.html")
    
    if os.path.exists(html_path):
        return FileResponse(html_path, media_type="text/html")
    
    return {"message": "Ephyra Chatbot API v3.0.0 - RAG Edition"}

@app.get("/health")
async def health():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "version": "3.0.0-RAG",
        "architecture": "Retrieval Augmented Generation with OpenAI GPT-4o-mini"
    }

@app.post("/ask")
@limiter.limit("30/minute")
async def ask(request: Request, body: AskBody):
    """
    Main endpoint: Implements full RAG pipeline
    1. RETRIEVE: Get context from semantic + keyword search
    2. AUGMENT: Format context
    3. GENERATE: Use Gemini to create natural response
    """
    conn = None
    current_lang = body.lang or detect_user_lang(body.messages[-1].content if body.messages else "")
    question = (body.messages[-1].content if body.messages else "").strip()

    try:
        if not question:
            return {
                "answer": "âŒ No question received" if current_lang == 'en' else "âŒ Î”ÎµÎ½ Î­Î»Î±Î²Î± ÎµÏÏÏ„Î·ÏƒÎ·",
                "quality": "error"
            }
        
        # Check for direct answers
        direct_answer = get_direct_answer(question)
        if direct_answer:
            log.info(f"âœ“ Direct answer matched: {question}")
            return direct_answer
        
        # Check for capabilities question
        if is_capabilities_question(question):
            log.info(f"â„¹ï¸ Capabilities question detected: {question}")
            capabilities = get_capabilities_response(current_lang)
            return {
                "answer": capabilities,
                "quality": "capabilities",
                "context_found": True,
                "confidence": 1.0
            }
        
        # Check for greeting
        if is_greeting(question):
            if len(body.messages) <= 1:
                log.info(f"ğŸ‘‹ Greeting detected on first message: {question}")
                if current_lang == 'en':
                    greeting = "Hello! I'm Ephyra, the professional AI assistant for the Municipality of Corinth. How can I help you today? ğŸ˜Š"
                else:
                    greeting = "Î“ÎµÎ¹Î± ÏƒÎ±Ï‚! Î•Î¯Î¼Î±Î¹ Î· Î•Ï†ÏÏÎ±, Î· ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ® AI Î²Î¿Î·Î¸ÏŒÏ‚ Ï„Î¿Ï… Î”Î®Î¼Î¿Ï… ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½. Î ÏÏ‚ Î¼Ï€Î¿ÏÏ Î½Î± ÏƒÎ±Ï‚ Î²Î¿Î·Î¸Î®ÏƒÏ‰ ÏƒÎ®Î¼ÎµÏÎ±; ğŸ˜Š"
                
                return {
                    "answer": greeting,
                    "quality": "greeting",
                    "context_found": False,
                    "confidence": 1.0
                }
            else:
                log.info(f"ğŸ‘‹ Greeting detected (not first message, skipping greeting response)")
                pass
        
        # Check for out of scope
        if is_out_of_scope(question):
            log.info(f"â›” Out of scope: {question[:50]}")
            if current_lang == 'en':
                msg = ("I'm sorry, I only assist with questions about the Municipality of Corinth. "
                      "For other topics, please ask something related to municipal services.")
            else:
                msg = ("Î›Ï…Ï€Î¬Î¼Î±Î¹, Î²Î¿Î·Î¸Ï Î¼ÏŒÎ½Î¿ Î¼Îµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ Ï„Î¿ Î”Î®Î¼Î¿ ÎšÎ¿ÏÎ¹Î½Î¸Î¯Ï‰Î½. "
                      "Î“Î¹Î± Î¬Î»Î»Î± Î¸Î­Î¼Î±Ï„Î±, Ï€Î±ÏÎ±ÎºÎ±Î»Ï ÏÏ‰Ï„Î®ÏƒÏ„Îµ ÎºÎ¬Ï„Î¹ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Î¼Îµ Ï„Î¹Ï‚ Î´Î·Î¼Î¿Ï„Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚.")
            
            return {
                "answer": msg,
                "quality": "out_of_scope",
                "context_found": False,
                "confidence": 0.0
            }
        
        # ==================== RAG PIPELINE ====================
        # Î‘Î½Î¿Î¯Î³Î¿Ï…Î¼Îµ Ï„Î· ÏƒÏÎ½Î´ÎµÏƒÎ· ÎœÎ•Î£Î‘ ÏƒÏ„Î¿ try
        conn = get_db_conn()
        cursor = conn.cursor()
        
        # Step 1: RETRIEVE context
        context_docs = retrieve_context(cursor, question, top_k=body.top_k)
        
        # ÎšÎ»ÎµÎ¯Î½Î¿Ï…Î¼Îµ Ï„Î¿Î½ cursor Î±Î¼Î­ÏƒÏ‰Ï‚ Î¼ÎµÏ„Î¬ Ï„Î·Î½ Î±Î½Î¬ÎºÏ„Î·ÏƒÎ·
        cursor.close()
        
        # Step 3: GENERATE answer
        answer, metadata = await generate_answer_with_rag(
            question, 
            context_docs, 
            current_lang,
            body.messages
        )
        
        # Calculate final confidence
        confidence = metadata.get('avg_similarity', 0.0) if context_docs else 0.0
        
        return {
            "answer": answer,
            "quality": "generated",
            "context_found": len(context_docs) > 0,
            "confidence": float(confidence),
            "documents_used": metadata.get('documents_used', 0),
            "sources": metadata.get('sources', [])
        }
    
    except Exception as e:
        error_msg_full = f"âŒ ERROR in /ask: {str(e)}"
        log.exception(error_msg_full)
        print(f"\nğŸ”´ CRITICAL ERROR: {error_msg_full}")
        print(f"   Exception type: {type(e).__name__}")
        print(f"   Question was: {question if 'question' in locals() else 'N/A'}")
        import traceback
        traceback.print_exc()
        
        if current_lang == 'en':
            error_msg = "An unexpected error occurred. Please try again."
        else:
            error_msg = "Î Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎµ Î±Ï€ÏÏŒÏƒÎ¼ÎµÎ½Î¿ ÏƒÏ†Î¬Î»Î¼Î±. Î Î±ÏÎ±ÎºÎ±Î»Ï Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î¾Î±Î½Î¬."
        
        return {
            "answer": error_msg,
            "quality": "error",
            "context_found": False,
            "confidence": 0.0
        }
    
    finally:
        # Î— ÎµÏ€Î¹ÏƒÏ„ÏÎ¿Ï†Î® Ï„Î·Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ Î³Î¯Î½ÎµÏ„Î±Î¹ Î Î‘ÎÎ¤Î‘ ÎµÎ´Ï
        if conn:
            return_db_conn(conn)
            log.info("ğŸ”Œ Connection returned to pool successfully.")

@app.post("/feedback")
@limiter.limit("10/minute")
async def record_feedback(request: Request):
    """Record user feedback."""
    conn = None
    try:
        data = await request.json()
        conn = get_db_conn()
        cursor = conn.cursor()
        
        conversation_id = data.get("conversation_id", str(uuid.uuid4()))
        bot_response = data.get("bot_response", "")
        user_question = data.get("user_question", "")
        is_positive = data.get("is_positive")
        
        if is_positive is None:
            return {"status": "error", "message": "is_positive field required"}
        
        user_agent = request.headers.get("User-Agent", "Unknown")
        client_ip = request.client.host if request.client else "Unknown"
        
        cursor.execute("""
            INSERT INTO chatbot_feedback 
            (conversation_id, bot_response, user_question, is_positive, user_agent, ip_address)
            VALUES (%s, %s, %s, %s, %s, %s)
        """, (
            conversation_id,
            bot_response[:5000],
            user_question[:2000] if user_question else None,
            is_positive,
            user_agent[:255],
            client_ip
        ))
        
        conn.commit()
        feedback_type = "âœ… Positive" if is_positive else "âŒ Negative"
        log.info(f"ğŸ“Š Feedback recorded: {feedback_type}")
        
        return {
            "status": "success",
            "message": "Thank you for your feedback!",
            "feedback_id": conversation_id
        }
    
    except Exception as e:
        log.exception(f"âŒ Feedback error: {e}")
        if conn:
            conn.rollback()
        return {"status": "error", "message": str(e)}
    
    finally:
        if conn:
            return_db_conn(conn)

@app.get("/tts_play")
@limiter.limit("15/minute")
async def tts_play(request: Request, text: str = ""):
    """Text-to-Speech endpoint."""
    text = (text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="Missing text parameter")
    if len(text) > 5000:
        raise HTTPException(status_code=400, detail="Text too long for TTS")
    
    audio_data = await _elevenlabs_tts_request(text)
    return StreamingResponse(io.BytesIO(audio_data), media_type="audio/mpeg")

@app.post("/tts_play")
@limiter.limit("15/minute")
async def tts_play_post(request: Request, body: TTSBody):
    """Text-to-Speech endpoint (POST)."""
    text = (body.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="Missing text")
    if len(text) > 5000:
        raise HTTPException(status_code=400, detail="Text too long for TTS")
    
    audio_data = await _elevenlabs_tts_request(text)
    return StreamingResponse(io.BytesIO(audio_data), media_type="audio/mpeg")



async def _elevenlabs_tts_request(text: str) -> bytes:
    if not ELEVENLABS_API_KEY:
        raise HTTPException(status_code=400, detail="API Key missing")
    
    try:
        # 1. ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î±Ï€ÏŒ Emojis (Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ ÏŒÏ€Ï‰Ï‚ Ï€ÏÎ¹Î½)
        clean_text = text.replace("ğŸ“", "").replace("ğŸ•’", "").replace("ğŸ“", "").replace("ğŸ“§", "").strip()
        
        # 2. Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î·Î»ÎµÏ†ÏÎ½Ï‰Î½ (Ï€.Ï‡. 2741024444 -> 2 7 4 1 0 2 4 4 4 4)
        # Î‘Ï…Ï„ÏŒ Î±Î½Î±Î³ÎºÎ¬Î¶ÎµÎ¹ Ï„Î·Î½ ElevenLabs Î½Î± Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ ÏˆÎ·Ï†Î¯Î¿-ÏˆÎ·Ï†Î¯Î¿
        clean_text = re.sub(r'(\d)', r'\1 ', clean_text)
        
        # 3. Î•Î¹Î´Î¹ÎºÎ® Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· Î³Î¹Î± Ï„Î¿ "24Ï‰ÏÎ¿" Î® "8:00" Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
        clean_text = clean_text.replace(" : ", ":") # Î•Ï€Î±Î½Î±Ï†Î¿ÏÎ¬ ÏÏÎ±Ï‚ Î±Î½ Ï‡Î¬Î»Î±ÏƒÎµ Î±Ï€ÏŒ Ï„Î± ÎºÎµÎ½Î¬
        
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id=ELEVENLABS_VOICE_ID,
            text=clean_text,
            model_id="eleven_multilingual_v2",
            output_format="mp3_44100_128"
        )
        
        audio_data = b""
        for chunk in audio_stream:
            if chunk: audio_data += chunk
        return audio_data

    except Exception as e:
        log.error(f"âŒ ElevenLabs Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
    
    
@app.on_event("shutdown")
def shutdown_event():
    """Cleanup on shutdown."""
    try:
        conn_pool.closeall()
        log.info("ğŸ”Œ Database connection pool closed")
    except Exception:
        pass

# ================== FEEDBACK ENDPOINTS ==================

@app.get("/feedback/stats")
async def get_feedback_stats(days: int = 30, detailed: bool = False):
    """Get feedback statistics with advanced metrics."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        since_date = datetime.now() - timedelta(days=days)
        
        # Basic stats
        cursor.execute("""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN is_positive THEN 1 ELSE 0 END) as positive,
                SUM(CASE WHEN is_positive THEN 0 ELSE 1 END) as negative
            FROM chatbot_feedback
            WHERE timestamp >= %s
        """, (since_date,))
        
        result = cursor.fetchone()
        total = result[0] or 0 if result else 0
        positive = result[1] or 0 if result else 0
        negative = result[2] or 0 if result else 0
        
        satisfaction_rate = round((positive / total * 100)) if total > 0 else 0
        
        # Daily data for chart
        cursor.execute("""
            SELECT 
                DATE(timestamp) as date,
                is_positive,
                COUNT(*) as count
            FROM chatbot_feedback
            WHERE timestamp >= %s
            GROUP BY DATE(timestamp), is_positive
            ORDER BY date
        """, (since_date,))
        
        daily_data = []
        for row in cursor.fetchall():
            if row:
                daily_data.append({
                    "date": str(row[0]) if row[0] else "",
                    "sentiment": "positive" if row[1] else "negative",
                    "count": row[2] or 0
                })
        
        # Top questions
        cursor.execute("""
            SELECT user_question, COUNT(*) as count
            FROM chatbot_feedback
            WHERE timestamp >= %s AND user_question IS NOT NULL AND user_question != ''
            GROUP BY user_question
            ORDER BY count DESC
            LIMIT 5
        """, (since_date,))
        
        top_questions = [{"question": row[0] or "", "count": row[1] or 0} for row in cursor.fetchall()]
        
        # Language distribution
        cursor.execute("""
            SELECT COUNT(*) as count
            FROM chatbot_feedback
            WHERE timestamp >= %s
        """, (since_date,))
        
        language_distribution = {"el": 100, "en": 0}
        
        # Sentiment trend
        prev_since = since_date - timedelta(days=days)
        cursor.execute("""
            SELECT 
                SUM(CASE WHEN is_positive THEN 1 ELSE 0 END) as positive,
                COUNT(*) as total
            FROM chatbot_feedback
            WHERE timestamp >= %s AND timestamp < %s
        """, (prev_since, since_date))
        
        prev_result = cursor.fetchone()
        prev_satisfaction = 0
        if prev_result and prev_result[1] and prev_result[1] > 0:
            prev_satisfaction = round((prev_result[0] / prev_result[1] * 100))
        
        sentiment_trend = satisfaction_rate - prev_satisfaction
        
        # User metrics
        cursor.execute("""
            SELECT COUNT(DISTINCT ip_address) as unique_users
            FROM chatbot_feedback
            WHERE timestamp >= %s
        """, (since_date,))
        
        user_result = cursor.fetchone()
        unique_users = user_result[0] if user_result and user_result[0] else 0
        
        # Recent feedback
        cursor.execute("""
            SELECT id, user_question, bot_response, is_positive, timestamp, ip_address
            FROM chatbot_feedback
            WHERE timestamp >= %s
            ORDER BY timestamp DESC
            LIMIT 50
        """, (since_date,))
        
        recent_feedback = []
        for row in cursor.fetchall():
            recent_feedback.append({
                "id": row[0],
                "user_question": row[1] or "",
                "bot_response": row[2] or "",
                "is_positive": row[3],
                "timestamp": row[4].isoformat() if row[4] else None,
                "ip_address": row[5],
                "language": "el",
                "response_time": 1.0
            })
        
        cursor.close()
        conn.close()
        
        return {
            "total_feedback": total,
            "positive": positive,
            "negative": negative,
            "satisfaction_rate": satisfaction_rate,
            "daily_data": daily_data,
            "top_issues": [],
            "top_questions": top_questions,
            "language_distribution": language_distribution,
            "sentiment_trend": sentiment_trend,
            "sentiment_trend_percent": f"+{sentiment_trend}%" if sentiment_trend > 0 else f"{sentiment_trend}%",
            "avg_response_time": 1.2,
            "min_response_time": 0.5,
            "max_response_time": 3.0,
            "unique_users": unique_users,
            "weekly_active_users": unique_users // 2,
            "recent_feedback": recent_feedback if detailed else []
        }
        
    except Exception as e:
        log.error(f"Error getting feedback stats: {e}")
        return {"error": str(e)}
        
        top_issues = [{"category": row[0], "count": row[1]} for row in cursor.fetchall()]
        
        # Most frequent questions
        cursor.execute("""
            SELECT user_question, COUNT(*) as count
            FROM chatbot_feedback
            WHERE timestamp >= %s AND user_question IS NOT NULL AND user_question != ''
            GROUP BY user_question
            ORDER BY count DESC
            LIMIT 5
        """, (since_date,))
        
        top_questions = [{"question": row[0], "count": row[1]} for row in cursor.fetchall()]
        
        # Language distribution
        cursor.execute("""
            SELECT 
                CASE WHEN user_question ILIKE '%[Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰]%' THEN 'el' ELSE 'en' END as lang,
                COUNT(*) as count
            FROM chatbot_feedback
            WHERE timestamp >= %s
            GROUP BY lang
        """, (since_date,))
        
        lang_dist = {row[0]: row[1] for row in cursor.fetchall()}
        total_lang = sum(lang_dist.values())
        language_distribution = {k: round(v/total_lang*100) if total_lang > 0 else 0 
                                for k, v in lang_dist.items()}
        
        # Sentiment trend (compare with previous period)
        prev_since = since_date - timedelta(days=days)
        cursor.execute("""
            SELECT 
                SUM(CASE WHEN is_positive THEN 1 ELSE 0 END) as positive,
                COUNT(*) as total
            FROM chatbot_feedback
            WHERE timestamp >= %s AND timestamp < %s
        """, (prev_since, since_date))
        
        prev_row = cursor.fetchone()
        prev_satisfaction = round((prev_row[0] / prev_row[1] * 100)) if prev_row[1] > 0 else 0
        sentiment_trend = satisfaction_rate - prev_satisfaction
        
        # User metrics
        cursor.execute("""
            SELECT 
                COUNT(DISTINCT ip_address) as unique_users,
                COUNT(DISTINCT CASE WHEN timestamp >= NOW() - interval '7 days' THEN ip_address END) as weekly_active
            FROM chatbot_feedback
            WHERE timestamp >= %s
        """, (since_date,))
        
        user_row = cursor.fetchone()
        unique_users = user_row[0] if user_row[0] else 0
        weekly_active = user_row[1] if user_row[1] else 0
        
        # Recent feedback
        cursor.execute("""
            SELECT id, user_question, bot_response, is_positive, timestamp, ip_address, 
                   CASE WHEN user_question ILIKE '%[Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰]%' THEN 'el' ELSE 'en' END,
            FROM chatbot_feedback
            WHERE timestamp >= %s
            ORDER BY timestamp DESC
            LIMIT 50
        """, (since_date,))
        
        recent_feedback = [
            {
                "id": row[0],
                "user_question": row[1],
                "bot_response": row[2],
                "is_positive": row[3],
                "timestamp": row[4].isoformat() if row[4] else None,
                "ip_address": row[5],
                "language": row[6],
                "response_time": row[7]
            }
            for row in cursor.fetchall()
        ]
        
        cursor.close()
        conn.close()
        
        return {
            "total_feedback": total,
            "positive": positive,
            "negative": negative,
            "satisfaction_rate": satisfaction_rate,
            "daily_data": daily_data,
            "top_issues": top_issues,
            "top_questions": top_questions,
            "language_distribution": language_distribution,
            "sentiment_trend": sentiment_trend,
            "sentiment_trend_percent": f"+{sentiment_trend}%" if sentiment_trend > 0 else f"{sentiment_trend}%",
            "avg_response_time": avg_response_time,
            "min_response_time": min_response_time,
            "max_response_time": max_response_time,
            "unique_users": unique_users,
            "weekly_active_users": weekly_active,
            "recent_feedback": recent_feedback if detailed else []
        }
        
    except Exception as e:
        log.error(f"Error getting feedback stats: {e}")
        return {"error": str(e)}


@app.get("/feedback/export")
async def export_feedback(days: int = 30, format: str = "csv"):
    """Export feedback data."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        since_date = datetime.now() - timedelta(days=days)
        
        cursor.execute("""
            SELECT id, user_question, bot_response, is_positive, timestamp, ip_address
            FROM chatbot_feedback
            WHERE timestamp >= %s
            ORDER BY timestamp DESC
        """, (since_date,))
        
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if format == "csv":
            import io
            import csv
            
            output = io.StringIO()
            writer = csv.writer(output)
            writer.writerow(['ID', 'Î•ÏÏÏ„Î·ÏƒÎ·', 'Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·', 'Sentiment', 'Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±', 'IP'])
            
            for row in rows:
                writer.writerow([
                    row[0],
                    row[1] or '',
                    row[2] or '',
                    'ğŸ‘ Î˜ÎµÏ„Î¹ÎºÏŒ' if row[3] else 'ğŸ‘ Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ',
                    row[4].strftime('%Y-%m-%d %H:%M:%S') if row[4] else '',
                    row[5]
                ])
            
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="text/csv",
                headers={"Content-Disposition": "attachment; filename=feedback_export.csv"}
            )
        
    except Exception as e:
        log.error(f"Error exporting feedback: {e}")
        return {"error": str(e)}


@app.post("/feedback/clear")
async def clear_all_feedback():
    """Delete all feedback data."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Delete all feedback
        cursor.execute("DELETE FROM chatbot_feedback")
        conn.commit()
        
        log.warning("ğŸ—‘ï¸ ALL FEEDBACK DELETED")
        
        return {
            "status": "success",
            "message": "âœ… ÎŒÎ»Î± Ï„Î± feedback Î´Î¹Î±Î³ÏÎ¬Ï†Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!"
        }
        
    except Exception as e:
        log.error(f"Error clearing feedback: {e}")
        if conn:
            conn.rollback()
        return {"status": "error", "message": str(e)}
    
    finally:
        if conn:
            return_db_conn(conn)

if __name__ == "__main__":
    import uvicorn
    log.info("ğŸš€ Ephyra Chatbot v3.0.0 - Production RAG Edition starting...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
